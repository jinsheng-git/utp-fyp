# Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Dataset
df = pd.read_excel('Logistics_Data.xlsx')

# Display basic information and the first 5 rows
print('--- Dataset Info ---')
df.info()
print('\n--- First 5 Rows ---')
df.head()

# Extract Year, Month, Day, Hour, DayOfWeek from 'Timestamp'
df['Year'] = df['Timestamp'].dt.year
df['Month'] = df['Timestamp'].dt.month
df['Day'] = df['Timestamp'].dt.day
df['Hour'] = df['Timestamp'].dt.hour
df['DayOfWeek'] = df['Timestamp'].dt.dayofweek
df = df.drop(columns=['Timestamp'])

# Drop cols that caused data leakage
df = df.drop(columns=['Logistics_Delay_Reason'], errors='ignore')
df = df.drop(columns=[col for col in df.columns if 'Logistics_Delay_Reason' in col or 'Traffic_Status' in col], errors='ignore')

# One-Hot Encoding on categorical cols
df = pd.get_dummies(df, columns=['Asset_ID', 'Shipment_Status'], drop_first=True)

# Define Features (X) and Target (y)
X = df.drop('Logistics_Delay', axis=1)
y = df['Logistics_Delay']

# Feature ccaling (Normalisation)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split data into 80:20 train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train RF for feature selection
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialise and train the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

# RF feature selection itself
feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance Score': rf_model.feature_importances_
}).sort_values('Importance Score', ascending=False)

print('--- Top 10 Most Important Features ---')
print(feature_importances.head(10))
print('\n\n')

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance Score', y='Feature', hue='Feature', legend=False, data=feature_importances.head(10), palette='rocket')
plt.title('Top 10 Most Important Features')
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Select top 10 features from Random Forest
top_features = feature_importances['Feature'].head(10).tolist()
print(f'Selected features for LSTM: {top_features}')

# Create a new dataframe with only the selected features and the target
df_lstm = df[top_features + ['Logistics_Delay']]

# Feature scaling (Normalisation)
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(df_lstm.drop('Logistics_Delay', axis=1))
scaled_df = pd.DataFrame(scaled_features, columns=top_features, index=df_lstm.index)
scaled_df['Logistics_Delay'] = df_lstm['Logistics_Delay']

# Sequence construction
# This function converts our data into sequences for the LSTM
def create_sequences(data, features, target_col, time_steps=10):
    X, y = [], []
    for i in range(len(data) - time_steps):
        # Features for the sequence
        X.append(data[features].iloc[i:(i + time_steps)].values)
        # Target value at the end of the sequence
        y.append(data[target_col].iloc[i + time_steps])
    return np.array(X), np.array(y)

TIME_STEPS = 10 # Use the last 10 data points to predict the next one
X_seq, y_seq = create_sequences(scaled_df, top_features, 'Logistics_Delay', time_steps=TIME_STEPS)

# Chronological train-test split
# Make sure the split is at 80:20 and will not shuffle the sequence
split_idx = int(len(X_seq) * 0.8)
X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]
y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]

# Build and train LSTM
model_lstm = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25, activation='relu'),
    Dense(1, activation='sigmoid') # Sigmoid for binary classification
])

model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_lstm.summary()

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model_lstm.fit(
    X_train_seq, y_train_seq,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stopping],
    verbose=1
)

#LSTM model evaluation
loss, accuracy = model_lstm.evaluate(X_test_seq, y_test_seq)
print(f'\nLSTM Test Set Evaluation')
print(f'Loss: {loss:.4f}')
print(f'Accuracy: {accuracy:.4f}')

# Get predictions to show a confusion matrix
y_pred_lstm_prob = model_lstm.predict(X_test_seq)
y_pred_lstm = (y_pred_lstm_prob > 0.5).astype(int)

print('\nLSTM Classification Report')
print(classification_report(y_test_seq, y_pred_lstm))

print('\nConfusion Matrix')
print(confusion_matrix(y_test_seq, y_pred_lstm))
